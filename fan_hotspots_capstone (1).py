# -*- coding: utf-8 -*-
"""Fan_Hotspots_Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BBkWWdm6FGyY7WDA7LE4eKLZdcOuNmSC

# **Fan Hotspots Capstone:**

Project Questions-

- What zip codes should the NBA be targeting first for its campaigns to
increase the number of fans and revenue domestically?

- Given publicly available demographic and economic data identify zip codes
with the most opportunity for NBA in terms of new fans and revenue.

# **Data ingestion**
"""

#import packages

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

fp = "/content/drive/MyDrive/2024-03-06 NBA Fans by Zip.csv"
df1 = pd.read_csv(fp)

fp2 = "/content/drive/MyDrive/NBA Team Zipcodes.csv"
df2 = pd.read_csv(fp2)

df2 = df2.rename(columns=lambda x: x.strip())

df1['NBA_FAN_CONCENTRATION'] = df1['NBA_FAN_CONCENTRATION'].round(2)

df2.head()

df2['Zipcode']

df1['ZIP_CODE'].info()

# Assuming 'ZIP_CODE' is the column containing zip codes in df1 and 'Zipcode' is the column containing zip codes in df2
# Create a new column in df1 to store the opportunity level
df1['Opportunity_Level'] = 'Low Opportunity'

# List of NBA team cities
nba_team_cities = ['Atlanta', 'Boston', 'Brooklyn', 'Charlotte', 'Chicago', 'Cleveland', 'Dallas', 'Denver', 'Detroit', 'Golden State', 'Houston', 'Indiana', 'Los Angeles', 'Memphis', 'Miami', 'Milwaukee', 'Minnesota', 'New Orleans', 'New York', 'Oklahoma City', 'Orlando', 'Philadelphia', 'Phoenix', 'Portland', 'Sacramento', 'San Antonio', 'Toronto', 'Utah', 'Washington D.C.']

# Loop through each row in dataset 1
for index, row in df1.iterrows():
    # Check if the ZIP_CODE is in dataset 2 (NBA team zip codes)
    if row['ZIP_CODE'] in df2['Zipcode'].values:
        # Assign "High Opportunity" opportunity level if the zip code is in dataset 2
        df1.at[index, 'Opportunity_Level'] = 'High Opportunity'
    else:
        # Check if CITY is in the list of NBA team cities
        if row['CITY'] in nba_team_cities:
            # Assign "High Opportunity" opportunity level if CITY is in the list
            df1.at[index, 'Opportunity_Level'] = 'High Opportunity'
        else:
            # Check if STATE is in the specified list for "Medium Opportunity"
            if row['STATE'] in ['FL','OH','CA','OK','IL','NJ','NY','GA','TX','PA','MA','NC','NH','IN','MI','LA','CT','UT','MS','CO','AZ','RI','DC','OR']:
                # Assign "Medium Opportunity" opportunity level
                df1.at[index, 'Opportunity_Level'] = 'Medium Opportunity'
            else:
                # Check if NBA_FAN_CONCENTRATION > 0.50
                if row['NBA_FAN_CONCENTRATION'] > 0.50:
                    # Assign "High Opportunity" opportunity level if NBA_FAN_CONCENTRATION > 0.50
                    df1.at[index, 'Opportunity_Level'] = 'High Opportunity'

# Print the first few rows of df1 with the opportunity levels assigned
print(df1.head())

df1.head()

df1.tail()

df=df1

df[df['Opportunity_Level'] == 'High Opportunity']

df[df['Opportunity_Level'] == 'Medium Opportunity']

df[df['Opportunity_Level'] == 'Low Opportunity']

df.info()

df.isnull().sum()

rows_with_null = df[['DMA_NAME','MEDIAN_AGE','ESTIMATE_HOUSEHOLDS_TOTAL','ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME','ESTIMATE_HOUSEHOLDS_MEAN_INCOME']]

rows_with_null[rows_with_null.isnull() == True]

df[['DMA_NAME','MEDIAN_AGE','ESTIMATE_HOUSEHOLDS_TOTAL','ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME','ESTIMATE_HOUSEHOLDS_MEAN_INCOME']].describe()

df['MEDIAN_AGE'].median().round()

# Remove non-numeric characters from the column
df['ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME'] = df['ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME'].str.replace(',', '').str.replace('+', '')

# Convert the column to float
df['ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME'] = df['ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME'].astype(float)

df['ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME'].median()

df['ESTIMATE_HOUSEHOLDS_TOTAL'].mean()

df['ESTIMATE_HOUSEHOLDS_MEAN_INCOME'].mean().round()

df[df['DMA_NAME'].isnull()][['STATE', 'CITY', 'DMA_NAME']]

df['MEDIAN_AGE'].fillna(42, inplace=True)
df['ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME'].fillna(67030.0, inplace=True)
df['ESTIMATE_HOUSEHOLDS_TOTAL'].fillna(3806.0, inplace=True)
df['ESTIMATE_HOUSEHOLDS_MEAN_INCOME'].fillna(92644.0, inplace=True)
df['DMA_NAME'].fillna('N/A', inplace=True)

df.head()

df.info()

df.isnull().sum()

"""# **Data Visualization**"""

#Here we want to check the distribution of our numerical columns
#This gives us more insights about how the data is skewed per column
columns = list(df)[:]

df[columns].hist(figsize=(20,50),layout=(14,4))

plt.show()

#Now lets visualize the frequency of each State that was mentioned in the Dataset
from wordcloud import WordCloud

# Combine all names into a single string
all_names = ' '.join(df['STATE'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_names)

# Plotting
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud of State')
plt.axis('off')  # Turn off axis
plt.show()

#Now lets visualize the frequency of each DMA_NAME that was mentioned in the Dataset
from wordcloud import WordCloud

# Combine all names into a single string
all_names = ' '.join(df['DMA_NAME'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_names)

# Plotting
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud of City')
plt.axis('off')  # Turn off axis
plt.show()

df_numerical = df.select_dtypes(include=[np.number])
df_numerical.corr()

#Now lets visualize this in a heatmap

plt.subplots(figsize=(20,14))

sns.heatmap(df_numerical.corr().abs(),vmax=1,square=True,annot=True,cmap='RdYlGn')

plt.title("Correlation bewtween NBA Fan Zipcode Data")

plt.show()

#Now lets visualize the Opportunity_Level

df["Opportunity_Level"].value_counts().plot(kind='pie',ylabel='frequency',autopct='%1.1f%%')

"""# **Data Preprocessing**"""

#Check Data Types

df.dtypes

# There are a couple Null values but we must see if we wan to keep them or replace and delete them moving forward

df.isnull().sum()

"""# **Encoding**"""

from sklearn.preprocessing import OneHotEncoder

# Assuming df is your original DataFrame containing all features including the categorical ones
# Extract the non-categorical features from df and store them in a separate DataFrame
non_categorical_features = df.drop(['STATE', 'CITY', 'DMA_NAME'], axis=1)


# Concatenate the original non-categorical features with the encoded categorical features
X = pd.concat([non_categorical_features], axis=1)

# Print the first few rows of X to verify
X = X.drop(['Opportunity_Level'],axis=1)

X

y = df['Opportunity_Level']
y

#Now we can label and fit y
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

y

#le.classes_ breaks up the labels into an array for us
#to view to see if it got the correct labels

le.classes_

"""# **Splitting X and y**"""

#import train_test_split from scikit learn
from sklearn.model_selection import train_test_split

#Now we need to start to train and test the AI model
#first thing we need to do is bring in our test_train_split module
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

X_train.shape

"""# **Modeling**"""

#Looks good now we can start using KNN Classificatoin
# Fitting clasifier to the Training set
# Loading libraries
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import cross_val_score

# Instantiate learning model (k = 7)
classifier = KNeighborsClassifier(n_neighbors=7)

X_train

# Fitting the model
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix,classification_report

#### 4.2 Evaluating predictions
cm = confusion_matrix(y_test,y_pred)
cm

#Confusion matrix may seem like an odd name, but it really serves to tell us how much confusion (error) there is in the model and what type of error.
pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)

#Here we will do a classification report
#A classificarion report is usally used in tandem with a confusion matrix
#it is used to give precision, recall, f-1 Score, and support
print(classification_report(y_test,y_pred))

#Time to plot the confusion matix
import seaborn as sns
sns.heatmap(cm, annot=True,fmt='g',cmap= 'Blues')
plt.xlabel('Predicted')
plt.ylabel('Truth')

#Calculating model accuracy
accuracy = accuracy_score(y_test, y_pred)*100
print('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')

#The accuracy is not bad at all but it can always be better
#k-fold cross-validation is one of the most popular strategies widely used by data scientists.
#This evaluates a models performance and will help us understand what K cvalue or neighbors we should have used in the model for better accuracy
#Using cross-validation for parameter tuning

# creating list of K for KNN
k_list = list(range(1,50,2))
# creating list of cv scores
cv_scores = []

# perform 10-fold cross validation
for k in k_list:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())

# changing to misclassification error
MSE = [1 - x for x in cv_scores]

plt.figure()
plt.figure(figsize=(15,10))
plt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')
plt.xlabel('Number of Neighbors K', fontsize=15)
plt.ylabel('Misclassification Error', fontsize=15)
sns.set_style("whitegrid")
plt.plot(k_list, MSE)

plt.show()

#Below on the chart it is showing us the optimal number of neighbors
#As you can seee below around 7 neihbors was the lowest
#The error rate started to spike after that

# finding best k
best_k = k_list[MSE.index(min(MSE))]
print("The optimal number of neighbors is %d." % best_k)

#The optimal number of neigbors was 7 which means that we entered  a good neighbor value
#below shows value 7

#Below is showing the error rate of k
error_rate = []

for i in range(1,50):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred = knn.predict(X_test)
    error_rate.append(np.mean(pred != y_test))

plt.figure(figsize=(12, 6))
plt.plot(range(1,50), error_rate, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')

#This is providing us with information on how the error rate is for kvalue
#looks like anything around the 3-5 range was okay
#Everything before and after were not good kvalues to initiate

import matplotlib.pyplot as plt

# Filter data for high and medium priority predictions
high_priority_indices = [i for i, prediction in enumerate(y_pred) if prediction == 0]
medium_priority_indices = [i for i, prediction in enumerate(y_pred) if prediction == 1]

# Extract corresponding data for high and medium priority predictions
high_priority_incomes = df1.iloc[high_priority_indices]['ESTIMATE_HOUSEHOLDS_MEAN_INCOME']
medium_priority_incomes = df1.iloc[medium_priority_indices]['ESTIMATE_HOUSEHOLDS_MEAN_INCOME']
high_priority_zipcodes = df1.iloc[high_priority_indices]['ZIP_CODE']
medium_priority_zipcodes = df1.iloc[medium_priority_indices]['ZIP_CODE']

# Scatter plot
plt.figure(figsize=(10, 5))

# Scatter plot for high priority predictions
plt.scatter(high_priority_incomes, [1] * len(high_priority_incomes), color='red', marker='o', label='High Opportunity', alpha=0.5)

# Scatter plot for medium priority predictions
plt.scatter(medium_priority_incomes, [0] * len(medium_priority_incomes), color='blue', marker='x', label='Medium Opportunity', alpha=0.5)

# Labels and title
plt.xlabel('Mean Household Income')
plt.yticks([0, 1], ['Medium Opportunity', 'High Opportunity'])
plt.title('Scatter Plot of Predicted Opportunity (High & Medium) vs. Mean Household Income')
plt.legend()

# Show plot
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Filter data for high priority predictions
high_priority_indices = [i for i, prediction in enumerate(y_pred) if prediction == 0]
high_priority_incomes = df1.iloc[high_priority_indices]['ESTIMATE_HOUSEHOLDS_MEAN_INCOME']
high_priority_zipcodes = df1.iloc[high_priority_indices]['ZIP_CODE']

# Scatter plot
plt.figure(figsize=(10, 8))
plt.scatter(high_priority_incomes, range(len(high_priority_incomes)), color='red', label='High Opportunity')

# Add zip codes as annotations
for i, txt in enumerate(high_priority_zipcodes):
    plt.annotate(txt, (high_priority_incomes.iloc[i], i), xytext=(5, -5), textcoords='offset points')

# Labels and title
plt.xlabel('Mean Household Income')
plt.ylabel('Predicted High Opportunity')
plt.title('Scatter Plot of Predicted High Opportunity vs. Mean Household Income')

# Show plot
plt.show()

import matplotlib.pyplot as plt

# Filter data for high priority predictions
high_priority_indices = [i for i, prediction in enumerate(y_pred) if prediction == 1]
high_priority_incomes = df1.iloc[high_priority_indices]['ESTIMATE_HOUSEHOLDS_MEAN_INCOME']
high_priority_zipcodes = df1.iloc[high_priority_indices]['ZIP_CODE']

# Filter incomes greater than 300000 and corresponding zip codes
high_priority_incomes = high_priority_incomes[high_priority_incomes > 300000]
high_priority_zipcodes = high_priority_zipcodes[:len(high_priority_incomes)]

# Scatter plot
plt.figure(figsize=(10, 8))
plt.scatter(high_priority_incomes, range(len(high_priority_incomes)), color='blue', label='Medium Opportunity')

# Add zip codes as annotations


# Labels and title
plt.xlabel('Mean Household Income')
plt.ylabel('Predicted Medium Opportunity')
plt.title('Scatter Plot of Predicted Medium Opportunity vs. Mean Household Income')

# Show plot
plt.show()

import matplotlib.pyplot as plt

# Filter data for high priority predictions
high_priority_indices = [i for i, prediction in enumerate(y_pred) if prediction == 0]
high_priority_incomes = df1.iloc[high_priority_indices]['ESTIMATE_HOUSEHOLDS_MEDIAN_INCOME']
high_priority_zipcodes = df1.iloc[high_priority_indices]['ZIP_CODE']

# Scatter plot
plt.figure(figsize=(10, 8))
plt.scatter(high_priority_incomes, range(len(high_priority_incomes)), color='red', label='High Opportunity')

# Add zip codes as annotations
for i, txt in enumerate(high_priority_zipcodes):
    plt.annotate(txt, (high_priority_incomes.iloc[i], i), xytext=(5, -5), textcoords='offset points')

# Labels and title
plt.xlabel('Median Household Income')
plt.ylabel('Predicted High Opportunity')
plt.title('Scatter Plot of Predicted High Opportunity vs. Median Household Income')

# Show plot

import matplotlib.pyplot as plt

# Filter data for high priority predictions
high_priority_indices = [i for i, prediction in enumerate(y_pred) if prediction == 0]
high_priority_incomes = df1.iloc[high_priority_indices]['NBA_FAN_CONCENTRATION']
high_priority_zipcodes = df1.iloc[high_priority_indices]['ZIP_CODE']

# Scatter plot
plt.figure(figsize=(10, 8))
plt.scatter(high_priority_incomes, range(len(high_priority_incomes)), color='red', label='High Opportunity')

# Add zip codes as annotations
for i, txt in enumerate(high_priority_zipcodes):
    plt.annotate(txt, (high_priority_incomes.iloc[i], i), xytext=(5, -5), textcoords='offset points')

# Labels and title
plt.xlabel('Fan Concentration')
plt.ylabel('Predicted High Opportunity')
plt.title('Scatter Plot of Predicted High Opportunity vs. Fan Concentration')

# Show plot

#List of Higest Opportunity Zipcodes for New Fan engagement and Revenue

high_priority_zipcodes = df1.iloc[high_priority_indices]['ZIP_CODE']



for item in high_priority_zipcodes:
    print(item)